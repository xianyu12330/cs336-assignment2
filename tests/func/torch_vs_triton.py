import torch
import math
import pandas as pd
import triton
import torch
from torch import Tensor
import triton.testing
from jaxtyping import Float, Bool, Int
from tests.func.flash_attn import FlashAttention2Triton,FlashAttention2_PyTorch
from cs336basics.cs336_basics.model import scaled_dot_product_attention

def run_benchmark():
    BATCH_SIZE = 1
    SEQ_LEN = [128,256,512,1024,2048,4096,8192,16384,32768,65536]
    HEAD_DIMS = [16,32,64,128]
    DTYPE = [torch.bfloat16,torch.float32]
    result = []
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒåŸºå‡†æµ‹è¯•... (é‡åˆ° OOM ä¼šè‡ªåŠ¨è·³è¿‡å¹¶è®°å½•)")
    #éå†æ‰€æœ‰ç»„åˆ
    for dtype in DTYPE:
        for dim in HEAD_DIMS:
            for seq_len in SEQ_LEN:
                print(f"Testing: SeqLen={seq_len}, Dim={dim}, Dtype={dtype}")
                # æ¯æ¬¡æµ‹è¯•å‰æ¸…ç©ºç¼“å­˜ï¼Œé˜²æ­¢å‰ä¸€æ¬¡æµ‹è¯•çš„å†…å­˜ç¢ç‰‡å¯¼è‡´æœ¬æ¬¡è¯¯æŠ¥ OOM
                torch.cuda.empty_cache()
                # --- 1. æ•°æ®å‡†å¤‡ ---
                # å¼€å¯ requires_grad=True ä»¥ä¾¿æµ‹è¯•åå‘ä¼ æ’­
                q = torch.randn(BATCH_SIZE, seq_len, dim,device='cuda',dtype=dtype,requires_grad=True)
                k = torch.randn(BATCH_SIZE, seq_len, dim, device='cuda',dtype=dtype,requires_grad=True)
                v = torch.randn(BATCH_SIZE, seq_len, dim, device='cuda',dtype=dtype,requires_grad=True)
                #æ¨¡æ‹Ÿä¸Šæ¸¸ä¼ æ¥çš„æ¢¯åº¦
                dO = torch.randn_like(q)
                # --- 2. ç”Ÿæˆå› æœæ©ç  (Causal Mask) ---
                # æ ¹æ®ä½ æä¾›çš„ scaled_dot_product_attention å‡½æ•°æ³¨é‡Šï¼š
                # "mask value of `False` should be masked out"
                # å› æœæ©ç æ„å‘³ç€æˆ‘ä»¬åªä¿ç•™ä¸‹ä¸‰è§’ï¼ˆå³ query_idx >= key_idx çš„éƒ¨åˆ†ï¼‰
                # æ³¨æ„ï¼šæˆ‘ä»¬åœ¨è®¡æ—¶å‡½æ•°å¤–éƒ¨ç”Ÿæˆ maskï¼Œé¿å…å°†ç”Ÿæˆ mask çš„å¼€é”€è®¡å…¥å»¶è¿Ÿ
                causal_mask = torch.tril(torch.ones(seq_len, seq_len, device='cuda', dtype=torch.bool))
                row = {
                    "Seq_len": seq_len,
                    "Dim": dim,
                    "Dtype": str(dtype).split('.')[-1]
                }
                #-------2------åŒ…è£…æµ‹è¯•å‡½æ•°
                #ã€pytorchã€‘ç‰ˆæœ¬
                def pytorch_fwd():
                    return scaled_dot_product_attention(q,k,v,mask=causal_mask)

                def pytorch_bwd():
                    out = scaled_dot_product_attention(q,k,v,mask=causal_mask)
                    out.backward(dO,retain_graph=True)

                def pytorch_e2e():
                    q.grad,k.grad,v.grad = None,None,None
                    out = scaled_dot_product_attention(q,k,v,mask=causal_mask)
                    out.backward(dO,retain_graph=True)

                #[triton]ç‰ˆæœ¬
                def triton_fwd():
                    return FlashAttention2Triton.apply(q,k,v,True)

                def triton_bwd():
                    out = FlashAttention2Triton.apply(q,k,v,True)
                    out.backward(dO,retain_graph=True)

                def triton_e2e():
                    q.grad,k.grad,v.grad = None,None,None
                    out = FlashAttention2Triton.apply(q,k,v,True)
                    out.backward(dO,retain_graph=True)

                #----3.æ‰§è¡ŒåŸºå‡†æµ‹è¯•å¹¶æ•æ‰ OOM ---
                def bench_with_oom_handling(fn):
                    try:
                        # do_bench è¿”å›çš„æ˜¯ä¸€ä¸ªä»¥æ¯«ç§’ä¸ºå•ä½çš„æ—¶é—´ (é»˜è®¤æ˜¯ä¸­ä½æ•°)
                        ms = triton.testing.do_bench(fn,quantiles = None)
                        print(f"{fn.__name__} : {ms:.3f}ms")
                        return f"{ms:.3f}ms"
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            return "OOM"
                        else:
                            raise e  # æŠ›å‡ºå…¶ä»–é OOM é”™è¯¯
                #å¼€å§‹æµ‹è¯•æ—¶é—´
                row['pytorch_fwd(ms)'] = bench_with_oom_handling(pytorch_fwd)
                row['triton_fwd(ms)'] = bench_with_oom_handling(triton_fwd)

                row['pytorch_bwd(ms)'] = bench_with_oom_handling(pytorch_bwd)
                row['triton_bwd(ms)'] = bench_with_oom_handling(triton_bwd)

                row['pytorch_e2e(ms) '] = bench_with_oom_handling(pytorch_e2e)
                row['triton_e2e(ms) '] = bench_with_oom_handling(triton_e2e)
                result.append(row)

    #4.æ±‡æ€»è¾“å‡º
    df = pd.DataFrame(result)
    print("\n============åŸºå‡†ç»“æœ===========")
    print(df.to_markdown(index=False))
    df.to_csv("flash_attention_benchmark.csv", index=False)  # ä¹Ÿå¯ä»¥ä¿å­˜ä¸º csv æäº¤
if __name__ == '__main__':
    run_benchmark()